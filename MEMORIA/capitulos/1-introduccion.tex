\chapter{Introducción}
\label{sec-1}


  Al igual que en el buffet de un restaurante, por mucho que se quisieran
  comer todos los platos favoritos, es imposible comer todo lo que uno
  quisiera por razones obvias. Una posibilidad es probar un poco de cada
  comida, para así saber qué es lo más delicioso y comer hasta
  hartarse.

  Pero, ¿qué hacer si hay demasiados platos y no se conocen todos? de
  alguna manera hay que saber cuáles hay que probar, si el objetivo es
  comer lo mejor posible. Un amigo puede recomendar una u otra comida,
  lo cual puede servir para orientarse. Entonces se pueden escoger
  pequeñas muestras de acuerdo a las recomendaciones.

  Complicando más el escenario, qué pasa si este restaurant tiene
  además música en vivo, y por alguna razón, se tiene el privilegio de
  escoger qué escuchar. En este caso, ya no es posible ``probar'' un
  poco de cada tipo existente, no sólo por la cantidad, sino porque no
  es posible juzgar un grupo musical por una canción o un extracto de
  ella. Si se quiere tener la mejor velada, pudiendo disfrutar de cada
  uno de los panoramas que ofrece, es necesario tener algo de
  información para poder escoger.

  Pasando a un contexto diferente, supóngase que este gran buffet es la
  Web y los distintos platos corresponden a contenido publicado en
  ella. Por lo tanto, dada la gran cantidad de información disponible,
  se hace necesario poder encontrar lo más atractivo de acuerdo a la
  preferencia del usuario o de los usuarios. Nótese que se está
  haciendo otra suposición importante con esta analogía, y es que se
  está considerando que la información es íntegramente para ser
  \emph{consumida}, y no, por ejemplo, para generar más contenido,
  conocimiento, o para ser utilizada por máquinas, etc. Dentro de
  este contexto se plantea la pregunta de cómo seleccionar el contenido
  más atractivo dentro de todo lo que hay disponible en un momento dado.

  Siguiendo el razonamiento de la analogía, una manera de poder
  seleccionar sólo el contenido más ``atractivo'' (de acuerdo a las
  preferencias del usuario), es probar un poco de cada uno. O bien,
  generar un \emph{resumen} de cada documento y presentarlo al usuario.

  Sin embargo, tal como se dijo antes, no es posible hacer lo anterior
  para contenido que no puede ser resumido directamente para ser
  consumido (la música como tal, o bien, vídeos o imágenes). En esta
  perspectiva, sólo las recomendaciones pueden ayudar a determinar lo
  más conveniente de acuerdo al usuario.

  Este trabajo consistió en el desarrollo de una primera aproximación
  que permite generar resúmenes automáticos de eventos bien definidos
  a partir de los documentos Web que hablan de éstos. Los documentos son
  considerados \emph{multimedios}: texto, imágenes, vídeos, sonidos; es
  decir, no necesariamente texto. El contenido es filtrado o
  seleccionado de acuerdo a indicadores sociales: los objetos más
  \emph{tomados en cuenta} en la Web son considerados más importantes.

  El sistema implementado consideró dos tipos de eventos: noticias y
  conciertos musicales. Para obtenerlos, se utilizó el servicio de
  Google News\footnote{\href{http://news.google.com}{http://news.google.com} } y
  Last.fm\footnote{\href{http://last.fm}{http://last.fm} }. Para medir la relevancia de los
  documentos y obtener los mismos se utilizó la red social
  Twitter\footnote{\href{http://twitter.com}{http://twitter.com} }, que provee una
  \emph{Aplication Programming Interface} (o API) para realizar búsquedas y
  obtener información sobre los \emph{tweets} o mensajes cortos que publican
  los usuarios de la red.

  La estructura de este informe es como sigue: en este capítulo se
  comenta el contexto dentro del cual se desarrolló este sistema, las
  contribuciones realizadas, los objetivos y una descripción general
  de la solución; en el siguiente capítulo se discute el estado del
  arte y el marco teórico del cual se desprende este trabajo; el
  capítulo 3 describe más en detalle el problema a resolver, su
  relevancia y sus dificultades, para luego, en el
  capítulo 4, describir la solución implementada, más un par de casos
  de estudio sobre los resultados obtenidos, para finalizar con las
  conclusiones de este trabajo en el capítulo 5.

\section{Contexto y Motivación}
\label{sec-1.1}


   La tasa de crecimiento de la cantidad de datos en la Web, y en
   particular, en las \emph{redes sociales online} (OSN, \emph{Online Social Networks}),
   es de tal magnitud que se vuelve necesario encontrar formas de
   filtrar y buscar sólo la información relevante dentro de todas las
   fuentes que hablan del mismo tópico o tema.

   En el contexto de las redes sociales online, cada día se publican
   millones de \emph{actualizaciones de estado} (mensajes breves sobre el
   estado actual del usuario) con respecto a distintos tópicos, ya
   sean conversacionales, personales o sobre algún evento en
   particular\footnote{Pear Analytics. Twitter Study \href{http://es.scribd.com/doc/18548460/Pear-Analytics-Twitter-Study-August-2009}{http://es.scribd.com/doc/18548460/Pear-Analytics-Twitter-Study-August-2009} }.
   Además, el auge de los teléfonos inteligentes o \emph{smartphones} con mayor
   capacidad de procesamiento e integrados con todo tipo de sensores
   (cámaras fotográficas, de vídeo, acelerómetro, barómetro,
   osciloscopio, etc.), hace posible el generar aun más información y
   e incluso en tiempo real sobre lo que acontece en el mundo, en
   internet, o bien sobre el estado particular de cada usuario.

   Este aumento y evolución de la generación de datos no sólo influye en la
   riqueza en la variedad de éstos, sino también en el
   comportamiento de los usuarios a lo largo del tiempo. Actualmente,
   una gran parte de los usuarios valora más el contenido de tipo
   multimedia (imágenes y videos) en las redes sociales online\footnote{The Rise of Visual Social Media \href{http://www.fastcompany.com/3000794/rise-visual-social-media}{http://www.fastcompany.com/3000794/rise-visual-social-media}. En el   artículo se menciona un estudio sobre comportamiendo y preferencias de los usuarios en las redes sociales hecho por ROI Research: \href{http://www.slideshare.net/performics_us/performics-life-on-demand-2012-summary-deck}{http://www.slideshare.net/performics\_us/performics-life-on-demand-2012-summary-deck} }.
   Se hace entonces necesario encontrar formas para satisfacer estas
   necesidades de los usuarios, las cuales ya han sido
   abordadas en parte, como la generación de
   resúmenes automáticos orientado a motores de búsqueda, o la
   determinación de la relevancia tanto de documentos en la Web como de
   actualizaciones de estado en las redes sociales.

   Surge como motivación el poder identificar y extraer contenido
   relevante de la Web, a partir de eventos, y además avanzar un
   paso más arriba en el nivel de abstracción: considerar los
   documentos no por su contenido textual, lo que permite abarcar
   imágenes, vídeos, sonidos y multimedios. Algunas
   aplicaciones directas de esto son, entre otras:

\begin{itemize}
\item Ayudar al trabajo periodístico mediante una colección de
     contenido multimedia relacionado a un evento noticioso. Por
     ejemplo, la versión online de Radio
     Biobío\footnote{\href{http://www.biobiochile.cl/}{http://www.biobiochile.cl/} } frecuentemente publica
     breves artículos sobre sucesos que tienen impacto en las redes
     sociales, mostrando un pequeño conjunto de mensajes con
     comentarios de la gente\footnote{Como muestra: \href{http://www.biobiochile.cl/2012/12/01/aporte-de-lustrabotas-de-santiago-a-la-teleton-provoca-admiracion-en-redes-sociales.shtml}{http://www.biobiochile.cl/2012/12/01/aporte-de-lustrabotas-de-santiago-a-la-teleton-provoca-admiracion-en-redes-sociales.shtml}, y \href{http://www.biobiochile.cl/2012/12/01/rechazo-provocan-condicionamientos-de-compra-de-ripley-y-unimarc-para-donar-a-la-teleton.shtml}{http://www.biobiochile.cl/2012/12/01/rechazo-provocan-condicionamientos-de-compra-de-ripley-y-unimarc-para-donar-a-la-teleton.shtml} }.
     Una aplicación directa involucraría
     considerar además contenido multimedia, y organizar este
     contenido de acuerdo a la relevancia que tiene dentro de las
     redes.
\item Enriquecer la búsqueda en la Web a través de contenido
     multimedia. Una persona buscando información sobre un concierto
     podría obtener imágenes y vídeos de éste fácilmente una vez
     identificado el concierto.
\item Siguiendo lo anterior, un grupo musical podría obtener toda la
     información multimedia asociada a su concierto, tanto para sus
     fans como para ellos mismos, potenciando su popularidad.
\item Poder distinguir entre eventos similares rápidamente. Por
     ejemplo, un usuario que desee obtener información sobre ``Gaza'',
     puede referirse tanto a la banda de música como al conflicto en
     Israel. El poder distinguir rápidamente mediante una imagen o un
     vídeo acelera mucho el proceso. \emph{Una imagen vale más que mil palabras}.
\end{itemize}
   El sistema implementado es una primera aproximación que puede
   satisfacer los ejemplos mencionados.

\section{Objetivos}
\label{sec-1.2}

\subsection{Objetivo general}
\label{sec-1.2.1}


    El objetivo principal de este trabajo fue el de poder evaluar e
    implementar en la práctica un sistema de extracción de contenido
    multimedia basado en la información social asociada a este
    contenido.

\subsection{Objetivos específicos}
\label{sec-1.2.2}


\begin{itemize}
\item Abstraerse del problema de identificación de eventos a partir de
      documentos Web, llevando a cabo una metodología de obtención de
      datos simple.
\item Implementar un modelo de \emph{clustering} para separar los
      documentos en \emph{subtópicos} de cada evento, sin considerar el
      tipo de contenido de estos documentos.
\item Analizar la efectividad del sistema implementado, evaluando
      casos de estudio.
\end{itemize}
\section{Descripción general de la solución}
\label{sec-1.3}


   Este trabajo puede considerarse como un punto de partida para el
   desarrollo de un modelo de recuperación de contenido multimedia,
   similar a lo que corresponde a la generación de resúmenes
   automáticos para múltiples documentos. En particular, se implementó
   un sistema que permite considerar distintas estrategias para
   continuar desarrollando en el futuro. Además:

\begin{itemize}
\item Se llevó a cabo una metodología para la obtención de documentos y
     enriquecerlos con datos obtenidos de fuentes sociales;
\item Se implementó un procedimiento que separar estos documentos en
     \emph{clusters}, \emph{sin considerar su contenido}. Sólo se utilizó la
     información social asociada; y
\item Se implementó además una forma de \emph{rankear} u ordenar los
     resultados de acuerdo a \emph{relevancia}, siendo ésta medida de
     acuerdo a la información social asociada a los documentos
     generados.
\end{itemize}
   El sistema implementado puede dividirse en tres componentes
   principales:
\begin{enumerate}
\item La que obtiene descripciones de eventos a partir de fuentes de
      éstos en la Web, enriqueciéndolos con información social;
\item Otra componente que procesa y separa los documentos a partir de
      la información social; genera \emph{objetos Web} y los separa en
      subtópicos de cada evento, respectivamente; y
\item La componente que entrega los $k$ documentos más relevantes por
      cada evento obtenido, basándose en los subtópicos identificados.
\end{enumerate}
   Se utilizaron las API de Google News como de Last.fm para la
   obtención de eventos: noticias y conciertos, respectivamente. Para
   el enriquecimiento de los eventos se utilizó la información social
   que provee Twitter y su API de búsqueda de \emph{tweets}. De la misma
   forma, se consideraron los metadatos de los mismos mensajes para medir
   la relevancia de los documentos generados.

   Un documento es identificado por la URL que lo ubica en la Web. El
   contenido no es más que la concatenación de los tweets que
   mencionan al documento. Se realizó una limpieza y preprocesamiento
   de los datos, quitando las \emph{stopwords} y realizando \emph{stemming}
   sobre el contenido en texto. Luego, se aplicó \emph{tf-idf} sobre los
   documentos, representándolos como vectores en el \emph{space vector    model}. Para identificar los subtópicos de un evento se utilizó el
   algoritmo de clustering $k$-means sobre los vectores.

   Para el ranking de los documentos se decidió usar una ponderación
   simple sobre una serie de indicadores que dependen de los tweets y
   de las URLs de cada evento.

   Entre las herramientas utilizadas, se usó lenguaje de
   programación Python, varias librerías para el manejo de datos
   (tales como \texttt{nltk}, \texttt{scipy}, \texttt{scikit-learn}, por nombrar las más
   importantes), el sistema de almacenamiento Redis, entre otras
   herramientas que son mencionadas en la descripción detallada de la
   solución.