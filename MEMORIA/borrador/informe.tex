% Created 2012-12-04 Tue 01:53
\documentclass[upright, contnum]{umemoria}
\usepackage[latin1]{inputenc}
\inputencoding{latin1}
%\usepackage[T1]{fontenc}
%\usepackage{fontspec}
\usepackage{graphicx}
%\defaultfontfeatures{Mapping=tex-text}
%\setmainfont{Linux Libertine O}
%\setmonofont[Scale=0.8]{DejaVu Sans Mono}

\let\evensidemargin\oddsidemargin
\reversemarginpar

\pagestyle{empty}



\title{informe}
\author{Mauricio}
\date{04 December 2012}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\vspace*{1cm}

\chapter{Descripción de la Solución}
\label{sec-1}


  La solución implementada consiste en escoger una
  representación de documentos que no considere el contenido de
  éstos. Por otra parte, la relevancia es medida utilizando la
  información social asociada a esta representación.

  Para esto, se asume que existe una fuente de eventos y documentos
  asociados preexistente, de esta forma, el enfoque de la solución
  radica principalmente en la generación de los resúmenes. Sin
  embargo, para la implementación sí fue necesario enfrentar el
  problema de identificar eventos, usando un enfoque práctico para
  obtenerlos.

  La figura FIGURA muestra de manera general el modelo propuesto.

  FIGURA MODELO

  DESCRIPCION DEL MODELO

  En la sección SECCION se describe el modelo utilizado para la
  implementación, con una especificación detallada de la solución. En la
  sección SECCION se describe la metodología de desarrollo y la
  implementación práctica realizada para representar el modelo
  formal, la cual considera la obtención de datos dentro del
  proceso. Luego se comentan los problemas técnicos que fueron
  enfrentados para terminar en la sección SECCION, donde se discuten
  un par de casos de estudio sobre los resultados obtenidos.

\section{Descripción detallada}
\label{sec-1.1}


   Para enfrentar el problema descrito se decidió utilizar una
   representación apropiada de los documentos que permita abstraerse
   de su contenido, utilizando la información social asociada a
   éstos. A partir de un evento determinado, se identifican los
   subtópicos del evento utilizando los documentos, y luego, para cada
   subtópicose determinan los documentos más relevantes utilizando
   esta información social.

   Para esto, fue necesario contar con dos \emph{fuentes de datos}: una
   fuente de eventos y de \emph{contenido social}, en la forma de mensajes
   y actualizaciones de estado.

   Se asumió que estas fuentes satisfacen los siguientes
   requerimientos:

\begin{itemize}
\item La fuente de eventos debe entregar una lista de eventos, los
     cuales consisten en los siguientes datos:

\begin{itemize}
\item Un título del evento y \emph{términos asociados}. Los términos
       asociados son breves frases o palabras que describan al
       evento, como por ejemplo, tags o etiquetas.
\item Como datos opcionales: breve descripción del evento, fecha de
       inicio y término, ubicación y direcciones Web.
\end{itemize}

\item La o las fuentes de contenido social deben entregar una lista de
     mensajes, con algunos metadatos tales como la fecha de creación,
     si el mensaje fue compartido, etc. Además, algunos datos sobre el
     autor del mensaje, como la cantidad de conexiones en la red, y
     en general, datos que permitan comparar dos autores.
\end{itemize}
   Utilizando estas dos fuentes, el modelo consistió en enriquecer los
   eventos obtenidos utilizando las fuentes sociales, generando
   documentos del tipo $d = (s_1, s_2, \ldots, s_m)$, donde 
   $s_i, i \in [1..m]$ es un mensaje de alguna fuente social, con los
   metadatos asociados. El documento es identificado por la URI de
   algún documento en la Web, de forma que todos los mensajes que
   contengan una URI en particular, corresponderán al mismo documento
   $d$.

   A continuación, utilizando alguna representación adecuada (vector
   space model, bag of words, etc.), se generaron clusters de documentos
   de un mismo evento, identificando los subtópicos. Con ellos fue
   posible generar un resumen que abarcara todos los aspectos del
   evento, en contraste con seleccionar directamente los documentos
   más relevantes del evento en su conjunto, lo cual puede dejar
   puntos de vista sin ser considerados por su extensión.

   Finalmente se seleccionan los $k$ documentos más representativos de
   cada cluster, utilizando como criterio los metadatos de los
   mensajes de la fuente social. De esta forma, se ordenan los
   documentos dejando como más ``relevantes'' los que más interés atrae
   de los usuarios.

\section{Metodología de desarrollo e implementación}
\label{sec-1.2}

  

\subsection{Obtención de datos}
\label{sec-1.2.1}

    Se describe a continuación el proceso diseñado para la obtención de
    datos para alimentar al sistema implementado.

    Las etapas de generación del dataset son las siguientes:

\begin{itemize}
\item Recolección de eventos (noticias y conciertos);
\item Enriquecimiento de los eventos existentes mediante tweets; e
\item Identificación de documentos a partir de los tweets por cada evento.

      Se recolectaron datos (eventos y tweets) desde el 19 de noviembre de
      2012 hasta el 30 de noviembre del mismo año, todos los días
      desde la medianoche hasta que el proceso termina exitosamente.
\end{itemize}
\subsubsection{Recolección de eventos}

Se consideraron dos tipos de eventos para el sistema: noticias y
conciertos musicales. Los conciertos incluyen festivales de varios
artistas.

\begin{itemize}
\item Noticias
  Para obtener las noticias, se utilizó el servicio de Google
  News\footnote{\href{http://news.google.com}{http://news.google.com} }. Existe una API (en proceso de
  obsolescencia, pero funcional a la fecha de este trabajo) que permite
  obtener no sólo los titulares y breve descripción de cada noticia,
  sino también un conjunto de entre 4-10 noticias relacionadas de otras
  fuentes. Esto sirvió para alimentar los términos de búsqueda para la
  etapa siguiente. Se guardaron los siguientes datos de una noticia:

\begin{itemize}
\item Título,
\item Descripción,
\item URL de la fuente, y
\item Titulares de las noticias relacionadas.
\end{itemize}

\item Conciertos
  Utilizando el servicio de Last.fm para obtener los conciertos y
  festivales de una ubicación en
  particular\footnote{\href{http://www.lastfm.es/api/show/geo.getEvents}{http://www.lastfm.es/api/show/geo.getEvents} }, se
  obtuvieron los conciertos y festivales de las siguientes
  ubicaciones:

\begin{itemize}
\item Santiago, Chile;
\item Londres, Inglaterra;
\item Glastonbury, Inglaterra;
\item Las Vegas, Nevada, EE.UU.; y
\item Estocolmo, Suecia.
\end{itemize}

\item Título del evento (concierto o festival);
\item Artistas que participan; y
\item Fechas de inicio y término (esta última no siempre está como
    dato).

  Además de otros datos descriptivos, como la ubicación, descripción
  breve, sitio web de la banda o festival, etc.
\end{itemize}
Cada vez que se obtienen los eventos se vuelven a obtener los
conciertos, pero sólo agregando los nuevos. Las noticias siempre son
nuevas, aun así por implementación no se consideraron los repetidos.
  
\subsubsection{Enriquecimiento de eventos}

Se obtuvieron tweets utilizando el servicio de búsqueda que provee
Twitter en su
API\footnote{\href{https://dev.twitter.com/docs/api/1.1/get/search/tweets}{https://dev.twitter.com/docs/api/1.1/get/search/tweets} }. El
objetivo es enriquecer los eventos con la información social que hay
en la Web sobre éstos. 

Para cada uno de los eventos obtenidos en la fase anterior, se
utilizaron los términos de búsqueda asociados a ellos: los titulares
de las noticias relacionadas y los nombres de los artistas para los
eventos noticiosos y musicales, respectivamente.

\begin{itemize}
\item Para las noticias, se hace una búsqueda en Twitter de los titulares
  al mismo tiempo en que se obtienen de Google News, y nuevamente al
  día siguiente, es decir, 2 búsquedas por cada titular de un evento.
  Se quitan las tildes y caracteres no ASCII y las stopwords, para
  evitar problemas con la implementación y no hacer calce de stopwords
  en la búsqueda de Twitter, respectivamente.
\item Para los conciertos y festivales, se utilizaron los nombres de los
  artistas y del evento como términos de búsqueda. De acuerdo a la
  información asociada al evento, se busca por una mayor cantidad de
  días:

\begin{itemize}
\item Se busca desde un día antes de inicio del evento;
\item Si está presente la fecha de término del evento, se busca cada día
    dentro del intervalo ``fecha de inicio'' a ``fecha de término'' hasta
    tres días terminado el evento.
\item Si no está presente la fecha de término (por ejemplo, un concierto
    o un festival de un día), se busca hasta tres días pasada la fecha
    de inicio.
\end{itemize}

\end{itemize}
\subsubsection{Identificación de documentos a partir de tweets}

    Luego de obtener los tweets asociados a cada evento, el siguiente
    paso fue generar los documentos que fueron usados para la
    generación de los resúmenes. Nuevamente, el modelo consistió en que cada
    documento se modeló como un vector de palabras, donde el
    identificador del documento es una URL, y sus componentes
    corresponden al contenido de los tweets que tienen esa URL en el
    texto del mensaje.

    El caso en el que un tweet no tenía ninguna URL en su contenido
    fue abordado de la siguiente forma: la URL asociada es una tal que
    representa al mismo tweet (utilizando el servicio de Twitter), y
    el contenido de ese documento es el mismo tweet, de forma de no
    dejar el tweet sin ser representado.

    Este proceso fue abordado recorriendo todos los eventos del
    dataset, observando todos los tweets asociados a cada evento,
    extrayendo la URL si es que hay alguna y guardando el documento
    con el nuevo tweet. Se marcan los tweets observados para no tener
    que repetir el proceso, ya que es intensivo en conexión a la red.

    Dada la condición breve de los mensajes publicados en la red
    social, muchos de los usuarios y/o servicios que publican mensajes
    con una URL n su interior suelen utilizar \emph{acortadores} (\emph{url shorteners})
    para los enlaces, y así no utilizar mucho espacio dentro de un
    mensaje. Otra ventaja que ofrecen es que algunos servicios como
    \hyperref[sec-1.2.1]{bit.ly} dan estadísticas sobre los visitantes a estos enlaces (y
    así saber quiénes vienen de cierta red social u otra, por
    ejemplo). Twitter, a su vez, actualmente también ofrece
    acortamiento de URLs por defecto. Esto suele producir que un enlace
    acortado se resuelva a otro enlace también acortado, por lo que es
    necesario resolver la URL completa para evitar duplicados o
    \emph{pseudo-duplicados} (en el caso en que dos URLs sintácticamente
    distintas apunten al mismo recurso). EN LA FIGURA\ldots{}...

    FIGURA DE LINKS CORTOS

    Por lo anterior, una vez identificada la URL del texto de un
    tweet, se resuelve su URL completa (que puede ya serlo de
    antemano), lo que consume recursos de ancho de banda y
    tiempo. 


\subsection{Generación de resúmenes}
\label{sec-1.2.2}


\subsection{Ranking de documentos}
\label{sec-1.2.3}


\section{Desafíos técnicos}
\label{sec-1.3}

\subsection{Restricciones de la API de Twitter}
\label{sec-1.3.1}


   La API de búsqueda de Twitter permite obtener tweets de acuerdo a un
   término de búsqueda. Se utilizó este servicio para enriquecer los
   eventos con información social utilizando como términos de búsqueda
   tanto los títulos de las noticias como los nombres de los artistas
   para las noticias y los conciertos, respectivamente. 
   
   Funciona de la siguiente forma: cada vez que se hace un request a la
   URL dada por el servicio, éste retorna a lo más 100 tweets por página, con un
   máximo de 15 páginas (indicando en el request qué página queremos
   consultar), dando como total hasta 1500 tweets por búsqueda. Existirán
   términos de búsqueda que no presenten ningún resultado  (ya sea por
   estar mal escritos o simplemente que no sean un tópico de discusión), o por
   el contrario, que se generen más tweets que los retornados por la
   búsqueda por cada ventana de tiempo que demore ésta (por ejemplo, un
   \emph{trending topic} o tópico que sea muy mencionado en la red social).
   
   Existe una limitación de uso de este servicio: sólo es posible hacer
   hasta 180 requests por cada 15 minutos, o 1 request cada 5
   segundos. Además, sólo retorna tweets de hasta 7 días de antigüedad, y
   sus resultados no son necesariamente en tiempo real y su estabilidad
   varía de acuerdo a factores externos.
   
   Los tweets retornados vienen en formato \texttt{JSON} (\emph{Javascript Simple Object Notation}),
   e incluyen varios metadatos sobre el tweet aparte de los principales,
   como autor, fecha, contenido. Algunos de estos metadatos son:
   
\begin{itemize}
\item Cantidad de \emph{retweets} hechos hasta la fecha;
\item Si posee alguna URL o \emph{hashtag} en el texto;
\item Si es una \emph{mención} a otro usuario;
\item La ubicación de donde se envió el tweet;
\item etc.
\end{itemize}
  Además, incluye datos sobre el autor, como por ejemplo:

\begin{itemize}
\item Si la cuenta está \emph{verificada};
\item La cantidad de seguidores del usuario;
\item Cantidad de amigos (seguidores que también lo siguen);
\item Cantidad de tweets;
\item Su descripción, y si incluye alguna URL, etc;
\item Ubicación (dada por el mismo usuario);
\item Fecha de creación de la cuenta;
\item etc.
\end{itemize}
\section{Casos de estudio}
\label{sec-1.4}







\end{document}
